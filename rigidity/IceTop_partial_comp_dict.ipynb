{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95d6387-9e82-450d-b01c-a4b21aa9b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os, sys, glob, pickle, tables\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "# pip install simweights <--- run this once to install the package\n",
    "# then replace <username> with your username and <version> with your Python version\n",
    "sys.path.append('/home/<username>/.local/lib/python3.<version>/site-packages')\n",
    "import simweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673ddfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to h5 files\n",
    "PATHS = ['/data/ana/CosmicRay/IceTop_level3/sim/IC86.2012/SIBYLL2.1/p/12360_v1s/h5files/*.h5',\n",
    "'/data/ana/CosmicRay/IceTop_level3/sim/IC86.2012/SIBYLL2.1/He/12630_v1s/h5files/*.h5',\n",
    "'/data/ana/CosmicRay/IceTop_level3/sim/IC86.2012/SIBYLL2.1/O/12631_v1s/h5files/*.h5',\n",
    "'/data/ana/CosmicRay/IceTop_level3/sim/IC86.2012/SIBYLL2.1/Fe/12362_v1s/h5files/*.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dea77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important functions\n",
    "def weighting(path):\n",
    "    weighter = None\n",
    "    filelist = glob.glob(path)\n",
    "\n",
    "    for filename in filelist:\n",
    "        file_obj = tables.open_file(filename, 'r')\n",
    "    \n",
    "        if weighter is None:\n",
    "            weighter = simweights.IceTopWeighter(file_obj)\n",
    "        else:\n",
    "            weighter += simweights.IceTopWeighter(file_obj)\n",
    "    return weighter\n",
    "\n",
    "def weighted_quantiles(values, weights, quantiles=0.5):\n",
    "    i = np.argsort(values)\n",
    "    c = np.cumsum(weights[i])\n",
    "    return values[i[np.searchsorted(c, np.array(quantiles) * c[-1])]]\n",
    "\n",
    "def weighted_percentile(data, weights, percentile):\n",
    "    \"\"\"\n",
    "    Compute the weighted percentile of a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array-like, the data values.\n",
    "    - weights: array-like, same length as data.\n",
    "    - percentile: float between 0 and 100.\n",
    "    \n",
    "    Returns:\n",
    "    - The weighted percentile value.\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    weights = np.asarray(weights)\n",
    "    \n",
    "    # Sort data and weights by data\n",
    "    sorted_indices = np.argsort(data)\n",
    "    data_sorted = data[sorted_indices]\n",
    "    weights_sorted = weights[sorted_indices]\n",
    "\n",
    "    # Compute the cumulative sum of weights\n",
    "    cumulative_weights = np.cumsum(weights_sorted)\n",
    "    normalized_weights = cumulative_weights / cumulative_weights[-1]  # normalize to 1\n",
    "\n",
    "    # Find where the normalized cumulative weight exceeds the desired percentile\n",
    "    return np.interp(percentile / 100, normalized_weights, data_sorted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81daa7d-0bfd-48d0-96da-917488f9a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the SIBYLL2.1 particle sims\n",
    "weighter = None\n",
    "\n",
    "if os.path.isfile('pickles/energy.pkl'):\n",
    "    with open('pickles/energy.pkl', 'rb') as file:\n",
    "        primary_energy = pickle.load(file)\n",
    "else:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/hits.pkl'):\n",
    "    with open('pickles/hits.pkl', 'rb') as file:\n",
    "        hits = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/particle_type.pkl'):\n",
    "    with open('pickles/particle_type.pkl', 'rb') as file:\n",
    "        particle_type = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/beta_cut_pass.pkl'):\n",
    "    with open('pickles/beta_cut_pass.pkl', 'rb') as file:\n",
    "        beta_cut = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/event.pkl'):\n",
    "    with open('pickles/event.pkl', 'rb') as file:\n",
    "        event = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/laputop_frac.pkl'):\n",
    "    with open('pickles/laputop_frac.pkl', 'rb') as file:\n",
    "        laputop_frac = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/max_inside.pkl'):\n",
    "    with open('pickles/max_inside.pkl', 'rb') as file:\n",
    "        max_inside = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/max_signal_6.pkl'):\n",
    "    with open('pickles/max_signal_6.pkl', 'rb') as file:\n",
    "        max_signal_6 = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/neighbour_max_signal_4.pkl'):\n",
    "    with open('pickles/neighbour_max_signal_4.pkl', 'rb') as file:\n",
    "        neighbour_max_signal_4 = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/reco_pass.pkl'):\n",
    "    with open('pickles/reco_pass.pkl', 'rb') as file:\n",
    "        reco_pass = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/runs.pkl'):\n",
    "    with open('pickles/runs.pkl', 'rb') as file:\n",
    "        runs = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/standard_filter.pkl'):\n",
    "    with open('pickles/standard_filter.pkl', 'rb') as file:\n",
    "        standard_filter = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/station_density_pass.pkl'):\n",
    "    with open('pickles/station_density_pass.pkl', 'rb') as file:\n",
    "        station_density_pass = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/sub_event_stream.pkl'):\n",
    "    with open('pickles/sub_event_stream.pkl', 'rb') as file:\n",
    "        sub_event_stream = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/sub_event.pkl'):\n",
    "    with open('pickles/sub_event.pkl', 'rb') as file:\n",
    "        sub_event = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "\n",
    "if os.path.isfile('pickles/Hweights.pkl'):\n",
    "    with open('pickles/Hweights.pkl', 'rb') as file:\n",
    "        Hweights = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)\n",
    "                \n",
    "if os.path.isfile('pickles/Gweights.pkl'):\n",
    "    with open('pickles/Gweights.pkl', 'rb') as file:\n",
    "        sub_event = pickle.load(file)\n",
    "elif not weighter==None:\n",
    "    for path in PATHS:\n",
    "        weighter = weighting(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aa1d1-bfad-410c-9094-23e0d80f00cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define quality cuts\n",
    "quality_cut = max_inside * station_density_pass * standard_filter * reco_pass * neighbour_max_signal_4 * max_signal_6 * laputop_frac * beta_cut\n",
    "quality_cut = quality_cut.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d6a19c-b653-4e46-875d-7b9a91ab0c3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If there are missing pkl files, load them with weighter\n",
    "if not os.path.isfile('pickles/energy.pkl'):\n",
    "    primary_energy = weighter.get_column('MCPrimary', 'energy')\n",
    "if not os.path.isfile('pickles/particle_type.pkl'):\n",
    "    particle_type = weighter.get_column('MCPrimary', 'type')\n",
    "if not os.path.isfile('pickles/hits.pkl'):\n",
    "    hits = weighter.get_column('IceTopHLCSeedRTPulses_SnowUnAttenuated_info', 'nstrings')\n",
    "    \n",
    "if not os.path.isfile('pickles/Hweights.pkl'):\n",
    "    # (p, He, N, Al, Fe) version of this model.\n",
    "    fluxH = simweights.GaisserH4a_IT()\n",
    "\n",
    "    # Get the weights by passing the flux to the weighter\n",
    "    Hweights = weighter.get_weights(fluxH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c372e3e-71a6-4032-97ea-8c1ecbe350a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary set-ups\n",
    "CUTS = {\n",
    "    'proton': particle_type==2.21200000e+03,\n",
    "    'helium' : particle_type==1.00002004e+09,\n",
    "    'oxygen': particle_type==1.00008016e+09,\n",
    "    'iron': particle_type==1.00026056e+09,\n",
    "    'true': np.tile(True, len(primary_energy))\n",
    "}\n",
    "A = {\n",
    "    'proton': 1.0073,\n",
    "    'helium': 4.0026,\n",
    "    'oxygen': 16,\n",
    "    'iron': 55.845  \n",
    "}\n",
    "Z = {\n",
    "    'true': 0,\n",
    "    'proton': 1,\n",
    "    'helium': 2,\n",
    "    'oxygen': 8,\n",
    "    'iron': 26  \n",
    "}\n",
    "'''\n",
    "Tier 1: 2011-14 is 3≤n<5\n",
    "Tier 2: 2011 is 5≤n<10; 2012-2013 is 5≤n<9; 2014 is 5≤n<8\n",
    "Tier 3: 2011 is 10≤n<14, from 2012-13 9≤n<16; 2014-15 is 8≤n<15; \n",
    "Tier 4: 2011 is 14≤n, 2012-13 16≤n; 2014-15 is 15≤n; 2016-17 is 14≤n; 2018-19 is 13≤n; 2020-21 is 12≤n\n",
    "'''\n",
    "TIERS = {\n",
    "    'Tier 3': (8<=hits)*(hits<15),\n",
    "    'Tier 4': 15<=hits\n",
    "}\n",
    "COLORS = {\n",
    "    'true' : 'black',\n",
    "    'proton': 'red',\n",
    "    'helium': 'orange',\n",
    "    'oxygen': 'purple',\n",
    "    'iron': 'blue'\n",
    "}\n",
    "LABEL = {\n",
    "    'true' : 'Data',\n",
    "    'proton': 'P',\n",
    "    'helium': 'He',\n",
    "    'oxygen': 'O',\n",
    "    'iron': 'Fe'\n",
    "}\n",
    "WEIGHTS = {\n",
    "    'true' : {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'proton': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'helium': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'oxygen': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'iron': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)}\n",
    "}\n",
    "BINS = {\n",
    "    'true' : {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'proton': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'helium': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'oxygen': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)},\n",
    "    'iron': {'Tier1': np.arange(5, 8.5, .1), 'Tier 2': np.arange(5, 8.5, .1), 'Tier 3': np.arange(5, 8.5, .1), 'Tier 4': np.arange(5, 8.5, .1)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4271ee3-c328-4f84-8102-7d01954e8dfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graph Log10 Energy vs Log 10 weighted counts\n",
    "ncols = len(TIERS)\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (13 * ncols, 8), ncols=ncols)\n",
    "\n",
    "for tier, tier_cut, ax in zip(TIERS.keys(), TIERS.values(), axs):\n",
    "    print(tier)\n",
    "    print()\n",
    "    \n",
    "    for comp, comp_cut in zip(CUTS.keys(), CUTS.values()):\n",
    "        combined_cut = comp_cut * tier_cut * quality_cut\n",
    "        WEIGHTS[comp][tier], BINS[comp][tier], patches1 = ax.hist(np.log10(primary_energy)[combined_cut], bins=50, weights=Hweights[combined_cut], log=True, label = LABEL[comp], histtype='step', color=COLORS[comp])\n",
    "        \n",
    "        if comp=='proton' or comp=='iron' or comp=='true':\n",
    "            print(f'    {comp}')\n",
    "            print(f'        Median: {round(weighted_quantiles(primary_energy[combined_cut]/1000000, Hweights[combined_cut]), 2)} PeV')\n",
    "            print(f'        68%: {round(weighted_percentile(primary_energy[combined_cut]/1000000, Hweights[combined_cut], 16), 2)}-{round(weighted_percentile(primary_energy[combined_cut]/1000000, Hweights[combined_cut], 84), 2)} PeV')\n",
    "            print()\n",
    "\n",
    "    ax.set_xlabel('Energy')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title(f'Log10 Energy vs Log 10 Weighted Counts for {tier}')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Atomic Charge\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(13 * ncols, 8), ncols=ncols)\n",
    "\n",
    "for tier, tier_cut, ax in zip(TIERS.keys(), TIERS.values(), axs):\n",
    "    totZ = 0\n",
    "    totH = 0\n",
    "        \n",
    "    for comp, comp_cut in zip(CUTS.keys(), CUTS.values()):\n",
    "        if not comp=='true':\n",
    "            totZ += WEIGHTS[comp][tier] * np.log(Z[comp])\n",
    "            totH += WEIGHTS[comp][tier]\n",
    "            \n",
    "    meanZ = totZ / totH\n",
    "\n",
    "    ax.plot(BINS['true'][tier][:-1], meanZ, label = 'Mean Z')\n",
    "\n",
    "    ax.set_xlabel('log10(Energy/GeV)')\n",
    "    ax.set_ylabel('Mean log10(Z)')\n",
    "    ax.set_title(f'Mean logZ vs. Energy for {tier}')\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c674cf-fe2c-445f-843f-4fad0cf63666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Fraction vs. Energy\n",
    "fig, axs = plt.subplots(figsize=(13 * ncols, 8), ncols=ncols)\n",
    "\n",
    "for tier, tier_cut, ax in zip(TIERS.keys(), TIERS.values(), axs):\n",
    "    for comp in CUTS:\n",
    "        if not comp=='true':\n",
    "            ax.plot(BINS[comp][tier][:-1], WEIGHTS[comp][tier]/totH, label=LABEL[comp])\n",
    "\n",
    "    ax.set_xlabel('log10(Energy/GeV)')\n",
    "    ax.set_ylabel('Fraction of Particles')\n",
    "    ax.set_title(f'Particle Fraction vs. Energy for {tier}')\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740607d0-f921-4c74-b7e8-3d140f0c0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of the overall percentage of particles, ln(A), and average rigidity\n",
    "PCOMP = {\n",
    "    'Tier 3': {'proton': None, 'helium': None, 'oxygen': None, 'iron': None},\n",
    "    'Tier 4': {'proton': None, 'helium': None, 'oxygen': None, 'iron': None}\n",
    "}\n",
    "LNA = {\n",
    "    'Tier 3': {'ln(A)': None},\n",
    "    'Tier 4': {'ln(A)': None}\n",
    "}\n",
    "AVG_RIGIDITY = {\n",
    "    'Tier 3': {'proton': None, 'helium': None, 'oxygen': None, 'iron': None},\n",
    "    'Tier 4': {'proton': None, 'helium': None, 'oxygen': None, 'iron': None},\n",
    "}\n",
    "for tier, tier_cut in zip(TIERS.keys(), TIERS.values()):\n",
    "    for comp, comp_cut in zip(CUTS.keys(), CUTS.values()):\n",
    "        if not comp=='true':\n",
    "            combined_cut = comp_cut * tier_cut * quality_cut\n",
    "                \n",
    "            PCOMP[tier][comp] = round(len(primary_energy[combined_cut])/len(primary_energy[tier_cut * quality_cut]), 3)\n",
    "            AVG_RIGIDITY[tier][comp] = np.log10(np.average(primary_energy[combined_cut]/Z[comp]))\n",
    "\n",
    "    LNA[tier]['ln(A)'] = np.average(np.log(particle_type[tier_cut * quality_cut]))\n",
    "\n",
    "print('Partial Comp:')\n",
    "print()\n",
    "print(DataFrame(PCOMP))\n",
    "print()\n",
    "print('ln(A):')\n",
    "print(DataFrame(LNA))\n",
    "print()\n",
    "print('log10(Avg. Rigidity):')\n",
    "print()\n",
    "print(DataFrame(AVG_RIGIDITY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740607d0-f921-4c74-b7e8-3d140f0c0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxG = simweights.GlobalSplineFit_IT()\n",
    "\n",
    "print(fluxG.pdgids)\n",
    "print(fluxG.groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.3.0: v1.12.1",
   "language": "shell",
   "name": "py3-v4.3.0_v1.12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
